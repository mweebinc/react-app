---
description: This rule guides LLM agents on how to leverage Jest and Puppeteer for comprehensive end-to-end testing of React applications.
alwaysApply: false
---
# Project Testing Rule

## Core Testing Philosophy

**Jest + Puppeteer Testing**: Use Jest as the test runner with Puppeteer for browser automation. All tests are located in the `tests/` folder.

**End-to-End Testing with Real Data**: Implement comprehensive functional testing that validates user workflows through actual data inputs, form submissions, and business logic validation.

**Visual Testing Configuration**: Configure Puppeteer with `headless: false` for browser-controlled testing that allows visual observation of test execution.

## Jest + Puppeteer Usage Rules

### Jest Configuration
- **Create `jest.config.cjs`**: Create Jest configuration file in project root
- **Test Files**: Use `.test.js` extension in `tests/` folder
- **Test Structure**: Use `describe()` blocks for grouping, `test()` or `it()` for individual tests
- **Setup/Teardown**: Use `beforeEach()` and `afterEach()` for browser management
- **Async Testing**: Use `async/await` for Puppeteer operations
- **Module System**: Use CommonJS (`require()`) for simplicity since Jest uses CommonJS by default

### Puppeteer Integration
- **Browser Launch**: Launch browser in `beforeEach()`, close in `afterEach()`
- **Page Navigation**: Use `page.goto()` with `waitUntil: 'domcontentloaded'`
- **Element Selection**: Use `page.$()` for single elements, `page.$$()` for multiple
- **User Interactions**: Use `page.click()`, `page.type()`, `page.select()` for form interactions
- **Assertions**: Use Jest's `expect()` with Puppeteer's element properties

### Test File Template
```javascript
const puppeteer = require('puppeteer');

describe('Feature Name', () => {
  let browser, page;

  beforeEach(async () => {
    browser = await puppeteer.launch({ headless: false });
    page = await browser.newPage();
  });

  afterEach(async () => {
    await browser.close();
  });

  test('should perform specific action', async () => {
    await page.goto('http://localhost:3000');
    // Test implementation
  });
});
```

## Performance Optimization

**Visual Testing Configuration**: Tests should run with visual observation enabled for human monitoring and debugging. Optimize for reliability and visibility:
- **Headless Mode**: Use `headless: false` for visual observation of test execution
- **Zero Delays**: Set `slowMo: 0` to eliminate artificial delays
- **Fast Navigation**: Use `waitUntil: 'domcontentloaded'` instead of `networkidle0`
- **Minimal Waits**: Only wait for elements when necessary, avoid arbitrary timeouts
- **Parallel Execution**: Run independent test suites concurrently when possible
- **Wait Strategy**: Only wait for elements that are actually needed for the test to proceed. If a selector is available, proceed immediately.

## Testing Strategy

### Test Creation Pattern
Create focused test files in `tests/` folder following this hierarchy:

1. **Authentication Foundation**: `signup.test.js`, `signin.test.js`
2. **Feature-Specific**: `[feature-name].test.js` (one file per major feature)
3. **Integration**: `navigation.test.js`, `offline.test.js`

### Test Design Principles

**Single Responsibility**: Each test file tests ONE specific feature or functionality
- ✅ `signup.test.js` - Only user registration
- ❌ `authentication.test.js` - Both signup AND signin

**Real Data Validation**: Use realistic test data that mirrors production scenarios
- Include edge cases and boundary conditions
- Test form validation, business logic, and data persistence
- Validate both happy path and error scenarios

**Comprehensive Coverage**: Test all critical user workflows
- Authentication flows (signup/signin)
- Core application features
- Navigation and routing
- Offline capabilities and sync

### Test Implementation Approach

**Pre-Test Validation**: Before writing any test, verify that input names and button selectors match the actual page elements
- Inspect form inputs to ensure `name` attributes are correctly set
- Verify button text, classes, and IDs match the test selectors
- Confirm all interactive elements are accessible and properly identified
- This prevents test failures due to mismatched selectors and ensures reliable automation


**Error Handling**: Capture comprehensive error information
- Browser console errors and warnings
- Network request failures
- Form validation errors
- UI interaction failures

**Test Data Management**: Use consistent, realistic test data
- Default credentials for authentication
- Invalid data for validation testing
- Edge cases and boundary conditions

## Quality Assurance Standards

### Test Reliability
- Use stable selectors and wait strategies
- Implement proper error handling and cleanup
- Handle flaky tests appropriately
- Generate comprehensive reports with timestamps and metrics

### Test Maintenance
- Update test data to match application changes
- Add new test cases for new features
- Remove obsolete tests
- Optimize test performance

## Implementation Guidelines

### When to Create Tests
- After implementing new features
- Before major deployments
- When fixing critical bugs
- During refactoring efforts

### Test Execution Strategy
- **Run Tests**: Use `npm test` or `jest` command to execute all tests
- **Single Test**: Use `npm test -- [test-file-name]` to run specific test files
- **No Watch Mode**: run tests once and exit (prevents watch mode)
- **Environment**: Ensure development server is running on localhost:3000
- **Cleanup**: Clean up test data after execution

## Iterative Testing Loop

**Continuous Quality Assurance**: After tests are executed, follow this iterative loop until all tests pass without issues:

### Post-Test Analysis
1. **Observe Test Results**: Carefully review test output, error messages, and browser console logs
2. **Identify Root Cause**: Determine if failures are due to:
   - Test implementation issues (incorrect selectors, timing, data)
   - Application code bugs (functionality, UI, business logic)
   - Environment issues (server, dependencies, configuration)

### Problem Resolution Process
3. **Fix Test Issues**: If problems are in test code:
   - Update selectors to match actual DOM elements
   - Adjust timing and wait strategies
   - Correct test data and validation logic
   - Improve error handling and reporting

4. **Fix Application Code**: If problems are in application code:
   - Debug and resolve functionality issues
   - Fix UI/UX problems
   - Correct business logic errors
   - Address performance and reliability issues

### Continuous Loop
5. **Re-run Tests**: Execute the test suite again after fixes
6. **Validate Results**: Ensure all tests now pass and no new issues are introduced
7. **Repeat Until Success**: Continue the loop until:
   - All tests pass without errors
   - No console warnings or errors
   - Application functions correctly in all tested scenarios
   - No regressions are detected

### Quality Gates
- **Zero Tolerance**: No test failures are acceptable in production-ready code
- **Complete Coverage**: All critical user workflows must be validated
- **Stable Performance**: Tests must run consistently without flakiness
- **Clean Output**: No console errors or warnings during test execution

This iterative approach ensures that both test quality and application functionality reach production standards through continuous improvement and validation.